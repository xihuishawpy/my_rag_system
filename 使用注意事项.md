# RAG系统使用注意事项

## 🔑 API密钥和环境配置

### 必须配置的环境变量
```bash
# 通义千问API密钥（必需）
export DASHSCOPE_API_KEY="sk-xxxxxxxxxxxxxxxxxxxx"

# 可选：自定义API端点
export QWEN_BASE_URL="https://dashscope.aliyuncs.com/compatible-mode/v1"
```

### API密钥获取步骤
1. 访问[阿里云百炼平台](https://bailian.console.aliyun.com/)
2. 注册/登录阿里云账号
3. 开通百炼服务
4. 创建API密钥
5. 确保账户有足够余额

### 常见API问题
- **401错误**：API密钥无效或过期
- **429错误**：请求频率过高，需要降低并发
- **403错误**：账户余额不足或权限不够

## 🗄️ 向量数据库选择指南

### FAISS vs ChromaDB 详细对比

| 特性 | FAISS | ChromaDB |
|------|-------|----------|
| **性能** | 极高 | 中等 |
| **内存占用** | 低 | 中等 |
| **易用性** | 中等 | 高 |
| **索引类型** | 多种可选 | 固定HNSW |
| **数据持久化** | 手动管理 | 自动管理 |
| **并发性能** | 优秀 | 良好 |
| **大规模数据** | 优秀 | 一般 |

### 选择建议

#### 使用FAISS的场景：
- ✅ 文档数量 > 10万
- ✅ 对检索速度要求极高
- ✅ 有足够的技术能力管理索引
- ✅ 内存资源充足

#### 使用ChromaDB的场景：
- ✅ 文档数量 < 10万
- ✅ 追求易用性和稳定性
- ✅ 不想手动管理索引
- ✅ 对性能要求不是特别高

### 切换数据库注意事项

1. **数据备份**：切换前备份现有数据
2. **重新索引**：切换后需要重新构建索引
3. **配置调整**：根据数据库类型调整相关参数
4. **性能测试**：切换后进行性能对比测试

## 💾 存储和性能规划

### 存储空间需求

#### 向量存储
- **文档向量**：每1万个文档约100MB
- **FAISS索引**：比原始向量小20-30%
- **ChromaDB**：比原始向量大30-50%

#### 缓存文件
- **嵌入缓存**：`embeddings_cache/` - 可达数GB
- **重排序缓存**：`qa_cache/` - 通常数百MB
- **问答缓存**：`cache/` - 根据使用量增长

#### 建议存储配置
```bash
# 1万文档的存储需求
向量存储: 100MB
索引文件: 130MB
缓存文件: 500MB
日志文件: 50MB
总计: ~800MB

# 10万文档的存储需求  
向量存储: 1GB
索引文件: 1.3GB
缓存文件: 2GB
日志文件: 100MB
总计: ~4.5GB
```

### 内存使用优化

#### 内存需求评估
```python
# 基础内存需求
基础系统: 1GB
向量数据库: 文档数 × 向量维度 × 4字节
Web界面: 500MB
缓存: 1-2GB

# 示例：2万文档，1536维向量
向量内存 = 20000 × 1536 × 4字节 = 123MB
推荐总内存 = 4GB以上
```

#### 内存优化策略
1. **减少批处理大小**
2. **清理无用缓存**
3. **使用内存映射文件**
4. **定期重启服务**

## 🚀 性能优化最佳实践

### 检索性能优化

#### FAISS优化
```python
# config.py中的优化配置
FAISS_INDEX_TYPE = "IndexFlatIP"  # 小数据量
# FAISS_INDEX_TYPE = "IVFFlat"   # 大数据量

# 索引参数调优
if document_count > 100000:
    index_type = "IVFFlat"
    nlist = int(4 * math.sqrt(document_count))
```

#### ChromaDB优化
```python
# 批处理优化
batch_size = min(1000, document_count // 10)

# 查询优化
chroma_collection.query(
    query_embeddings=embeddings,
    n_results=top_k,
    include=["documents", "metadatas", "distances"]
)
```

### 检索参数调优

#### 基础参数
```python
# 检索数量 - 影响准确性和速度
TOP_K = 10          # 小数据量
TOP_K = 20          # 大数据量或复杂查询

# 重排序参数 - 影响最终质量
RERANK_TOP_K = 3    # 保守设置
RERANK_TOP_K = 5    # 平衡设置
```

#### 混合搜索权重
```python
# 语义搜索权重 (0.0-1.0)
HYBRID_ALPHA = 0.7  # 偏向语义搜索
HYBRID_ALPHA = 0.5  # 平衡搜索
HYBRID_ALPHA = 0.3  # 偏向关键词搜索
```

### 缓存策略

#### 启用多级缓存
```python
# 嵌入缓存 - 避免重复计算
embedding_cache_enabled = True
embedding_cache_size = 10000

# 重排序缓存 - 提升响应速度  
rerank_cache_enabled = True
rerank_cache_size = 5000

# 问答缓存 - 相同问题快速响应
qa_cache_enabled = True
qa_cache_size = 1000
```

#### 缓存清理策略
```bash
# 定期清理脚本
#!/bin/bash
# 清理7天前的缓存
find ./embeddings_cache -name "*.json" -mtime +7 -delete
find ./qa_cache -name "*.json" -mtime +7 -delete
find ./cache -name "*.json" -mtime +7 -delete
```

## 🔧 常见问题解决方案

### 启动问题

#### 1. 依赖包冲突
```bash
# 问题：ImportError或版本冲突
# 解决：重新安装
pip uninstall -y chromadb faiss-cpu llamaindex
pip install -r requirements.txt
```

#### 2. 端口被占用
```bash
# 问题：Web界面无法启动
# 解决：更换端口
streamlit run src/web_interface.py --server.port 8502
```

#### 3. 权限问题
```bash
# 问题：无法创建目录或文件
# 解决：检查权限
chmod 755 ./vector_store
chmod 755 ./cache
```

### 运行时问题

#### 1. 内存不足
```python
# 现象：系统卡顿或崩溃
# 解决：减少参数
CHUNK_SIZE = 256        # 减少文档块大小
TOP_K = 5               # 减少检索数量
batch_size = 100        # 减少批处理大小
```

#### 2. 检索结果质量差
```python
# 解决策略：
# 1. 增加检索数量
TOP_K = 20

# 2. 调整混合搜索权重
HYBRID_ALPHA = 0.8

# 3. 使用重排序
RERANK_TOP_K = 5

# 4. 优化数据处理
CHUNK_SIZE = 256
OVERLAP_SIZE = 64
```

#### 3. 响应速度慢
```python
# 优化策略：
# 1. 启用缓存
enable_all_caches = True

# 2. 减少检索数量
TOP_K = 10
RERANK_TOP_K = 3

# 3. 使用FAISS
VECTOR_STORE_TYPE = "faiss"
```

### API相关问题

#### 1. 请求超时
```python
# 增加超时时间
timeout = 60  # 秒

# 或添加重试机制
max_retries = 3
```

#### 2. 令牌限制
```python
# 减少上下文长度
max_context_length = 4000

# 或分批处理长文档
def split_long_documents(doc, max_length=2000):
    # 分割逻辑
    pass
```

## 🔍 监控和维护

### 性能监控指标

#### 关键指标
1. **响应时间**：问答完整流程时间
2. **检索精度**：检索结果相关性
3. **内存使用**：系统内存占用
4. **存储占用**：磁盘空间使用
5. **API调用**：API请求次数和费用

#### 监控方法
```python
# 在代码中添加监控
import time
import psutil

def monitor_performance():
    # 响应时间
    start_time = time.time()
    # ... 执行操作 ...
    response_time = time.time() - start_time
    
    # 内存使用
    memory_usage = psutil.Process().memory_info().rss / 1024 / 1024  # MB
    
    # 记录日志
    logger.info(f"响应时间: {response_time:.2f}s, 内存使用: {memory_usage:.1f}MB")
```

### 定期维护任务

#### 每日维护
```bash
#!/bin/bash
# 检查日志文件大小
log_size=$(du -m logs/rag_system.log | cut -f1)
if [ $log_size -gt 100 ]; then
    # 轮转日志
    mv logs/rag_system.log logs/rag_system.log.old
    touch logs/rag_system.log
fi

# 清理临时文件
find ./temp -name "*.tmp" -delete
```

#### 每周维护
```bash
#!/bin/bash
# 清理过期缓存
find ./embeddings_cache -name "*.json" -mtime +7 -delete
find ./qa_cache -name "*.json" -mtime +7 -delete

# 压缩旧日志
gzip logs/*.log.old

# 检查磁盘空间
df -h | grep -E "(vector_store|cache)"
```

#### 每月维护
```bash
#!/bin/bash
# 重建索引（可选）
python src/run_vector_store.py --rebuild

# 清理无用文件
find . -name "*.pyc" -delete
find . -name "__pycache__" -type d -exec rm -rf {} +

# 系统性能测试
python src/test_performance.py
```

## 🔐 安全和隐私

### 数据安全
1. **API密钥安全**：不要在代码中硬编码密钥
2. **数据备份**：定期备份向量数据库
3. **访问控制**：限制Web界面访问权限
4. **日志脱敏**：避免在日志中记录敏感信息

### 隐私保护
1. **数据本地化**：向量数据存储在本地
2. **API调用**：仅发送查询文本，不发送原始文档
3. **缓存清理**：定期清理包含敏感信息的缓存

## 📈 扩展和升级

### 水平扩展
1. **分布式部署**：多实例负载均衡
2. **数据分片**：按类别分割数据库
3. **缓存集群**：使用Redis等外部缓存

### 垂直扩展
1. **硬件升级**：增加内存和存储
2. **模型优化**：使用更好的嵌入模型
3. **算法改进**：优化检索和排序算法

---

## 📞 获取更多帮助

如果遇到本文档未涵盖的问题：

1. **查看日志**：`logs/rag_system.log`
2. **检查配置**：确认`src/config.py`设置
3. **测试组件**：运行相关的`test_*.py`文件
4. **性能诊断**：使用系统监控工具

记住：大多数问题都与API配置、内存不足或数据质量相关。 